\documentclass[12pt,oneside,ngerman,reqno,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsthm}     % ams stuff should be before font loading
\usepackage{lmodern}
\usepackage[T1]{fontenc}        % should be after font loading
\usepackage{fixltx2e,babel}
\usepackage[numbers]{natbib}    % bibtex package
%\usepackage{typearea}           % custom type area
%   \areaset[0mm]{135mm}{210mm}  % typearea configuration
%   \topmargin5mm                % typearea configuration
\usepackage{graphicx}
\usepackage{url}
\usepackage{booktabs}
\usepackage{algorithm} 
\usepackage{algpseudocode} 

\usepackage{ifpdf}
\ifpdf
  \pdfminorversion=5
  \pdfoutput=1
% Diese Pakete laufen anscheinend nur mit pdflatex gescheit
    \usepackage[bitstream-charter]{mathdesign}
    \usepackage[pdfusetitle,colorlinks=true,linktoc=all]{hyperref}
\else
    \usepackage[bitstream-charter]{mathdesign}
%    falls Mathdesign nicht geht, kann dieser Font genutzt werden.
%    \usepackage{times}
    \usepackage[dvips,ps2pdf,linktoc=all]{hyperref}
\fi

\ifdefined\hypersetup
  \hypersetup{
    pdfkeywords={}, linkcolor=black, citecolor=black, filecolor=black, urlcolor=black,
  }
\fi


\usepackage{fancyhdr}

% Das ist nur fuer den Blindtext zustaending
\usepackage{blindtext}


% Blattlayout
\textwidth15cm
\textheight23cm
\topmargin0cm
\oddsidemargin0.5cm
\evensidemargin0.5cm

% Einzug f"ur den Absatz
\parindent0em
\parskip0.5em
% Header, Footer
\usepackage{fancyhdr}
 % Text in Kopfzeile aussen
\fancyhead[LE,RO]{\nouppercase{\leftmark} }
% Text in Kopfzeile innen
\fancyhead[RE,LO]{}
% Text in Fusszeile mittig
\fancyfoot[CE,CO]{\thepage}
\pagestyle{fancy}

\begin{document}

\pagenumbering{Alph}
\renewcommand{\thepage}{C-\Roman{page}}
There is already some research in effectiveness of various step size rules for the subgradient method available. It is already known that the constant step size rule applied to the subgradient method results almost surely in convergence of the algorithm within a tolerance ($\liminf_{k\to \infty} \le L^* + \psi$, with $\psi \ge 0$ and $L^*=\min_{\lambda \ge 0} L(\lambda)$)\cite{StochasticSubgradientMethod_YaohuaHu}.  If we have convergence in that case, it converges at a linear rate. \cite{SubgradientMethod_Ratliff}. Whereby we also know that if we use a (nonsummable) diminishing step size rule, we can even guarantee exact convergence of the algorithm ($\liminf_{k\to\infty} = L^*$) \cite{StochasticSubgradientMethod_YaohuaHu}. Unfortunately it only converges at a sublinear rate and only when strong convexity of the function to minimize is given \cite{SubgradientMethod_Ratliff}. Which is why diminishing step size rules often lead to slow convergence near the eventual limit \cite{IncrementalGradientMethodConstant_Blatt}. We want to further show with computational experiments how good the different step size rules work for the subgradient method, especially regarding the level of accuracy of the solution. 
\end{document}